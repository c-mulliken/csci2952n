{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-mulliken/csci2952n/blob/main/mini_project_normalizing_flows.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stencil code for Mini Project Normalizing Flows. **Please make a copy before modifying the code.**\n",
        "\n",
        "Created by Calvin Luo (calvin_luo@brown.edu)"
      ],
      "metadata": {
        "id": "7MgUwPp7ewwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Set Up Code"
      ],
      "metadata": {
        "id": "os2PXsUVgVja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting, qol\n",
        "import imageio\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "\n",
        "# ML stuff\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow.keras.layers as tfkl\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "# Target distribution loader\n",
        "from sklearn.datasets import make_moons"
      ],
      "metadata": {
        "id": "m3_Y0LXpjvjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a source distribution - in this case, a simple 2D Gaussian will suffice.\n",
        "source_dist = tfp.distributions.MultivariateNormalDiag(loc=(0,0))\n",
        "\n",
        "# Define a noise value for the moons distribution (how much deviation from the two semi-circle lines)\n",
        "moon_noise = 0.1"
      ],
      "metadata": {
        "id": "0jjkXFUNVYoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to generate visualization GIFs.\n",
        "def generate_gif(frames, fname='temp.gif', time_per_frame=500, final_frame_time=2000):\n",
        "  # Pause the GIF at the end by extending the duration of the last frame\n",
        "  duration = [time_per_frame] * (len(rendered)-1) + [final_frame_time]\n",
        "  imageio.mimwrite(fname, rendered, loop=0, duration=duration)"
      ],
      "metadata": {
        "id": "-X3f28omeWra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Normalizing Flow Models\n",
        "\n",
        "There's nothing to edit in the first block, but give it a read and run it!\n",
        "`Transform` is just a base class for the actual networks to inherit from.\n",
        "\n",
        "In the second block, we will begin our implementation of an Affine Transformation Block.  Down the road, we will stack multiple of these for expressibility, creating a full normalizing flow implementation.\n",
        "\n",
        "The third block has implemented the Affine Flow for you, which stacks multiple `AffineTransformBlocks`, and computes the forward and inverse operations along with dimension shifting for expressibility.\n",
        "\n",
        "**Task 1.1** Implement the transformation functions of `AffineTransformBlock`.\n",
        "\n",
        "Fill in the `transform` function and `inverse_transform` appropriately.  They should implement an affine transformation and its inverse (where you can choose which is which).  As a reminder, an affine transform will use provided parameters to scale input $x$ and shift it.  Make sure to design the affine transformation in a way that guarantees invertibility.\n",
        "\n",
        "**Task 1.2** Implement an Affine Transform Block, in [Masked Autoregressive Flow (MAF)](https://arxiv.org/pdf/1705.07057) style where conditioning is performed on data samples for faster training.\n",
        "\n",
        "The conditioner is already provided as a LSTM that produces 2-dimensional outputs, which can be interpreted as your affine transformation parameters.\n",
        "\n",
        "Fill in the `forward` function, which maps Gaussian samples to data samples.  As per MAF, we will need to compute $x_i$ before utilizing it as subsequent conditioning; sampling is therefore autoregressive and slow.  We therefore must iteratively step through our LSTM calls, manipulating the outputs accordingly before passing it back in as input.\n",
        "\n",
        "Fill in the `inverse` function, which maps data samples to Gaussian samples.  As per MAF, we have access to the entire $x_i$ input needed to calculate the transformation parameters.  Technically this is still autoregressive (due to the conditioner being an LSTM), but we do not need to manipulate the outputs of the conditioner.  Do not forget to compute an accurate log of the determinant of the jacobian term!  Whereas the forward sampling pass doesn't need this, the inverse call does in order to facilitate maximum likelihood training."
      ],
      "metadata": {
        "id": "1amxuRNXV2Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Base Class\n",
        "\n",
        "class Transform:\n",
        "  def __init__(self):\n",
        "    self._conditioner = None\n",
        "\n",
        "  def transform(self, z, params):\n",
        "    pass\n",
        "\n",
        "  def inverse_transform(self, x, params):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x):\n",
        "    pass\n",
        "\n",
        "  def inverse(self, x):\n",
        "    pass"
      ],
      "metadata": {
        "id": "LqRat-j0dKPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoregressive Affine Flow"
      ],
      "metadata": {
        "id": "kpdw3Sat_sND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "AUTOREGRESSIVE FLOW SPECIFICATION\n",
        "  Transformer: Affine\n",
        "  Conditioner: LSTM\n",
        "'''\n",
        "\n",
        "class AffineTransformBlock(Transform):\n",
        "  def __init__(self):\n",
        "    # Conditioner only outputs 2 values, the weight and bias.\n",
        "    self._conditioner = tfkl.LSTM(2, return_sequences=True, return_state=True)\n",
        "\n",
        "  def transform(self, z, w, b):\n",
        "    # TODO (Step 1.1): implement an affine transformation\n",
        "    # Hint: exponentiate the weight to guarantee invertibility\n",
        "    pass\n",
        "\n",
        "  def inverse_transform(self, x, w, b):\n",
        "    # TODO (Step 1.1): implement inverse of the above affine transformation\n",
        "    # Hint: exponentiate the weight to guarantee invertibility\n",
        "    pass\n",
        "\n",
        "  def forward(self, z):\n",
        "    # Input z should be shape [b, dim]\n",
        "    # As a reminder, tfkl.LSTM takes in inputs of dimension 3: (batch, timesteps, feature)\n",
        "    # Return x should be shape [b, dim], representing the predicted transformed sample.\n",
        "    b, dim = z.shape\n",
        "    dtype = z.dtype\n",
        "\n",
        "    # Initial conditioning information is set to default as a constant (e.g. 1).\n",
        "    # You can modify this if you like, along with its initialization shape.\n",
        "    ar_input = tf.ones((b), dtype=dtype)\n",
        "    prev, state = self._conditioner.cell.get_initial_state(batch_size=b)\n",
        "\n",
        "    # TODO (Step 1.2): implement forward function\n",
        "    # As per MAF, write a loop that iteratively calculates the transformed x_i.\n",
        "    # Hint: look at the call function: https://github.com/keras-team/keras/blob/v3.3.3/keras/src/layers/rnn/lstm.py#L559\n",
        "    # to get a clue as to how to manually perform calls of your LSTM.\n",
        "    x = None\n",
        "    return x\n",
        "\n",
        "  def inverse(self, x):\n",
        "    # Input x should be shape [b, dim]\n",
        "    # As a reminder, tfkl.LSTM takes in inputs of dimension 3: (batch, timesteps, feature)\n",
        "    # Return z should be shape [b, dim], representing the predicted inverted sample.\n",
        "    b, dim = x.shape\n",
        "    dtype = x.dtype\n",
        "\n",
        "    # Initial conditioning information is set to default as a constant (e.g. 1).\n",
        "    # The rest should be the corresponding x dimensions, as per MAF.\n",
        "    ar_input = tf.concat([tf.ones((b, 1, 1), dtype=dtype), x[:,:-1,None]], axis=1)\n",
        "\n",
        "    # TODO (Step 1.2): implement inverse function\n",
        "    # As per MAF, compute affine parameters by autoregressively conditioning on inputs x.\n",
        "    # During inversion, we need to compute the log jacobian determinant, for subsequent use in likelihood estimation.\n",
        "    # Hint: do we need a for loop?  And can we compute the transformation all at once?\n",
        "    z = None\n",
        "    log_jacob_det = None\n",
        "    return z, log_jacob_det"
      ],
      "metadata": {
        "id": "4DhZio_cVoZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AffineFlow:\n",
        "  '''\n",
        "  Implements AffineFlow with arbitrary number of blocks,\n",
        "  and naively rolling the dimensions by one each step.\n",
        "  '''\n",
        "  def __init__(self, num_blocks):\n",
        "    self.blocks = [AffineTransformBlock() for _ in range(num_blocks)]\n",
        "\n",
        "  def forward(self, z):\n",
        "    for i in range(len(self.blocks)):\n",
        "      z = self.blocks[i].forward(tf.roll(z, 1, -1))\n",
        "    x = tf.roll(z, -i-1, -1)\n",
        "    return x\n",
        "\n",
        "  def inverse(self, x):\n",
        "    accumulated_ljd = 0\n",
        "    x = tf.roll(x, len(self.blocks)+1, -1)\n",
        "    for i in range(len(self.blocks)):\n",
        "      x, log_jacob_det = self.blocks[-i-1].inverse(tf.roll(x, -1, -1))\n",
        "      accumulated_ljd += log_jacob_det\n",
        "    z = tf.roll(x, -1, -1)\n",
        "    return z, accumulated_ljd\n",
        "\n",
        "  def get_weights(self):\n",
        "    return sum([block._conditioner.trainable_weights for block in self.blocks], [])"
      ],
      "metadata": {
        "id": "7cNb8wHxZnxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sanity Checks!\n",
        "\n",
        "We need to test two main things:\n",
        "\n",
        "\n",
        "1.   The `forward` and `inverse` functions are exact inverses of each other.\n",
        "2.   That the Jacobian Determinant computation is equivalent to the one computed via autodifferentiation.\n",
        "\n",
        "Applying the forward and inverse flows in sequence to a sample should implement the identity function.  The same argument holds for applying the inverse and forward flows in sequence.  We use this fact to verify the correctness of our implementations, for a random sample.\n",
        "\n",
        "The TensorFlow GradientTape can perform a batch Jacobian computation, which we can follow with a determinant calculation (though it is the expensive $O(D^3)$ version to handle general Jacobians).  We can compare this against the tractable Jacobian determinant computation in our implementation that leverages the autoregressive modeling design.\n",
        "\n",
        "The maximum difference for all these checks should be sufficiently small (e.g. < 1e-5)\n",
        "\n",
        "**[NOTE] Please do not proceed with the project until these sanity checks are satisfied.**"
      ],
      "metadata": {
        "id": "Qgl-kEjErNuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly initiate the parameters of an arbitrary Affine Autoregressive Flow for robustness.\n",
        "batch_size = 500\n",
        "dim = np.random.randint(10, 20)\n",
        "num_blocks = np.random.randint(5, 10)\n",
        "\n",
        "# Define our testing flow and random sample.\n",
        "test_flow = AffineFlow(num_blocks)\n",
        "random_sample = tf.random.normal((batch_size, dim))\n",
        "\n",
        "# Verify the flow's forward and inverse functions act as inverses of each other.\n",
        "forward_base = test_flow.forward(random_sample)\n",
        "inverted_base, _ = test_flow.inverse(forward_base)\n",
        "print(f'Max Diff Between Inverse of Forward Sample and Sample: {np.max(np.abs(inverted_base - random_sample))}')\n",
        "invert_target, _ = test_flow.inverse(random_sample)\n",
        "forward_target = test_flow.forward(invert_target)\n",
        "print(f'Max Diff Between Forward of Inverse Sample and Sample: {np.max(np.abs(forward_target - random_sample))}')\n",
        "\n",
        "# Verify that the jacobian is correctly computed.\n",
        "# We compare against a ground-truth computation, which is not as tractable as our direct computation.\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(random_sample)\n",
        "  inverted, log_det_jac = test_flow.inverse(random_sample)\n",
        "jacobian = tape.batch_jacobian(inverted, random_sample)\n",
        "gt_log_det_jac = tf.math.log(tf.abs(tf.linalg.det(jacobian)))\n",
        "print(f'Max Diff Between Computed Jacobian Determinant and Ground Truth Jacobian Determinant: {np.max(np.abs(gt_log_det_jac - log_det_jac))}')"
      ],
      "metadata": {
        "id": "dLl1ZJJx6ti4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Training Normalizing Flows\n",
        "\n",
        "**Task 2.1** Implement the missing loss computation below, to perform training.  Recall that with normalizing flows, we can compute likelihoods for the data sample under the current parameterization; we therefore can train our model using maximum likelihood directly!"
      ],
      "metadata": {
        "id": "98S_3CWo_9gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "tf.random.set_seed(1337)\n",
        "\n",
        "# Training hyperparameters; 20 blocks with 200 steps should solve the Moon_Target<=>Normal_Base problem.\n",
        "# 10 blocks is a decent approximation.\n",
        "# Please modify these parameters as you please.\n",
        "num_blocks = 20\n",
        "batch_size = 500\n",
        "iterations = 200\n",
        "lr = 0.05\n",
        "\n",
        "flow = AffineFlow(num_blocks)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# Training Loop\n",
        "pb = Progbar(iterations, stateful_metrics=['loss'])\n",
        "for i in range(iterations):\n",
        "  moon, _ = make_moons(batch_size, noise=moon_noise)\n",
        "\n",
        "  # TODO (Step 2.1): Please implement the loss computation, as maximum likelihood averaged over the batch.\n",
        "  # [NOTE:] you must not use the tape.batch_jacobian function!\n",
        "  # [NOTE:] determine how to appropriately utilize log_jacobian_det term - it can vary depending on your implementation choices above!\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = None\n",
        "  grads = tape.gradient(loss, flow.get_weights())\n",
        "  opt.apply_gradients(zip(grads, flow.get_weights()))\n",
        "  pb.add(1, values=[('loss', loss)])"
      ],
      "metadata": {
        "id": "ZjQa6hk_aDyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Visualizing Normalizing Flows\n",
        "\n",
        "The rest of this notebook is pure visualization of your learned normalizing flow model - there is nothing more to code up!  The below blocks behave as visual sanity checks - if you have implemented everything above correctly, then you should see some beautiful flow interpolations between source and target distributions!\n",
        "\n",
        "Run through these code blocks and marvel at the flows you've learned!"
      ],
      "metadata": {
        "id": "PP-QdOiVgbjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Forward Flow Path Visualization\n",
        "# Forward flowing of samples from the source distribution, with intermediate caching.\n",
        "sample = source_dist.sample(500)\n",
        "samples = [sample]\n",
        "for i in range(len(flow.blocks)):\n",
        "  sample = flow.blocks[i].forward(tf.roll(sample, 1, -1))\n",
        "  samples.append(tf.roll(sample, -i-1, -1))\n",
        "\n",
        "# Plotting Logic\n",
        "rendered = []\n",
        "for i in range(len(samples)):\n",
        "  fig, axes = plt.subplots(1, 2, sharex=True, sharey=True)\n",
        "  fig.set_size_inches(8, 4)\n",
        "  fig.tight_layout(pad=2)\n",
        "\n",
        "  ax = plt.subplot(1, 2, 1)\n",
        "  ax.set_title(f'Forward Flow Distribution, Step {i}')\n",
        "  sns.kdeplot(x=samples[i][:,0], y=samples[i][:,1], fill=True, cmap='PuBu')\n",
        "\n",
        "  ax = plt.subplot(1, 2, 2)\n",
        "  ax.set_title(f'Forward Flow Scatter, Step {i}')\n",
        "  ax.scatter(samples[i][:, 0], samples[i][:, 1])\n",
        "\n",
        "  fig.canvas.draw()\n",
        "  rendered.append(np.array(fig.canvas.renderer._renderer))\n",
        "  plt.close()\n",
        "\n",
        "fname = 'forward_flow.gif'\n",
        "generate_gif(rendered, fname)\n",
        "Image(fname)"
      ],
      "metadata": {
        "id": "Rcj4pxx169_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inverse Flow Path Visualization\n",
        "# Inverse flowing of samples from the target distribution, with intermediate caching.\n",
        "sample = tf.cast(moon, tf.float32)\n",
        "samples = [sample]\n",
        "sample = tf.roll(sample, len(flow.blocks)+1, -1)\n",
        "for i in range(len(flow.blocks)):\n",
        "  sample, _ = flow.blocks[-i-1].inverse(tf.roll(sample, -1, -1))\n",
        "  samples.append(tf.roll(sample, len(flow.blocks) - i, -1))\n",
        "\n",
        "# Plotting Logic\n",
        "rendered = []\n",
        "for i in range(len(samples)):\n",
        "  fig, axes = plt.subplots(1, 2, sharex=True, sharey=True)\n",
        "  fig.set_size_inches(8, 4)\n",
        "  fig.tight_layout(pad=2)\n",
        "\n",
        "  ax = plt.subplot(1, 2, 1)\n",
        "  ax.set_title(f'Inverse Flow Distribution, Step {i}')\n",
        "  sns.kdeplot(x=samples[i][:,0], y=samples[i][:,1], fill=True, cmap='PuBu')\n",
        "\n",
        "  ax = plt.subplot(1, 2, 2)\n",
        "  ax.set_title(f'Inverse Flow Scatter, Step {i}')\n",
        "  ax.scatter(samples[i][:, 0], samples[i][:, 1])\n",
        "\n",
        "  fig.canvas.draw()\n",
        "  rendered.append(np.array(fig.canvas.renderer._renderer))\n",
        "  plt.close()\n",
        "\n",
        "fname = 'inverse_flow.gif'\n",
        "generate_gif(rendered, fname)\n",
        "Image(fname)"
      ],
      "metadata": {
        "id": "7Kh0uXpA7auF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Forward Flow Comparison Against Ground Truth Target Samples\n",
        "# Visualize a forward pass from the trained model\n",
        "sample = source_dist.sample(500) # Normal samples\n",
        "forwarded = flow.forward(sample) # Forwarded samples\n",
        "moon, _ = make_moons(batch_size, noise=moon_noise) # True samples\n",
        "\n",
        "fig = plt.figure(figsize=(16, 8), frameon=False)\n",
        "\n",
        "ax = fig.add_subplot(2, 3, 1)\n",
        "ax.set_title('Source Samples')\n",
        "ax.scatter(sample[:,0], sample[:,1])\n",
        "\n",
        "ax = fig.add_subplot(2, 3, 2)\n",
        "ax.set_title('Source Samples Forward Flowed')\n",
        "ax.scatter(forwarded[:,0], forwarded[:,1])\n",
        "\n",
        "ax = fig.add_subplot(2, 3, 3)\n",
        "ax.set_title('Target Samples')\n",
        "ax.scatter(moon[:,0], moon[:,1])\n",
        "\n",
        "ax = fig.add_subplot(2, 3, 4)\n",
        "sns.kdeplot(x=sample[:,0], y=sample[:,1], fill=True, cmap='PuBu')\n",
        "\n",
        "ax = fig.add_subplot(2, 3, 5)\n",
        "sns.kdeplot(x=forwarded[:,0], y=forwarded[:,1], fill=True, cmap='PuBu')\n",
        "\n",
        "ax = fig.add_subplot(2, 3, 6)\n",
        "sns.kdeplot(x=moon[:,0], y=moon[:,1], fill=True, cmap='PuBu')\n",
        "\n",
        "fig.tight_layout(pad=1)\n",
        "fig.canvas.draw()"
      ],
      "metadata": {
        "id": "k6mR-NOV8MPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inverse Flow Comparison Against Ground Truth Source Samples\n",
        "# Visualize a forward pass from the trained model\n",
        "sample = source_dist.sample(500) # Normal samples\n",
        "moon, _ = make_moons(batch_size, noise=moon_noise) # True samples\n",
        "inverted, _ = flow.inverse(tf.cast(moon, tf.float32)) # Inverted samples\n",
        "\n",
        "fig = plt.figure(figsize=(16, 8), frameon=False)\n",
        "\n",
        "ax = fig.add_subplot(2, 3, 1)\n",
        "ax.set_title('Target Samples')\n",
        "ax.scatter(moon[:,0], moon[:,1])\n",
        "\n",
        "ax = fig.add_subplot(2, 3, 2)\n",
        "ax.set_title('Target Samples Inverse Flowed')\n",
        "ax.scatter(inverted[:,0], inverted[:,1])\n",
        "\n",
        "ax = fig.add_subplot(2, 3, 3)\n",
        "ax.set_title('Source Samples')\n",
        "ax.scatter(sample[:,0], sample[:,1])\n",
        "\n",
        "ax = fig.add_subplot(2, 3, 4)\n",
        "sns.kdeplot(x=moon[:,0], y=moon[:,1], fill=True, cmap='PuBu')\n",
        "\n",
        "ax = fig.add_subplot(2, 3, 5)\n",
        "sns.kdeplot(x=inverted[:,0], y=inverted[:,1], fill=True, cmap='PuBu')\n",
        "\n",
        "ax = fig.add_subplot(2, 3, 6)\n",
        "sns.kdeplot(x=sample[:,0], y=sample[:,1], fill=True, cmap='PuBu')\n",
        "\n",
        "fig.tight_layout(pad=1)\n",
        "fig.canvas.draw()"
      ],
      "metadata": {
        "id": "zv5zuG_zh_1U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}